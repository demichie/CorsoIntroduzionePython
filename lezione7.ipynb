{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfUDs86GpSqhT1BxbJIPkc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/demichie/CorsoIntroduzionePython/blob/main/lezione7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lezione 7: Manipolazione Dati Avanzata e Analisi Statistica**\n",
        "\n",
        "Nella lezione precedente abbiamo iniziato a esplorare la libreria **Pandas**, imparando a caricare dati tabulari, a ispezionare i `DataFrame` con comandi come `.head()` e `.info()`, e a eseguire le prime operazioni di pulizia e selezione.\n",
        "\n",
        "Oggi approfondiremo la nostra conoscenza di Pandas, passando da semplici selezioni a tecniche di manipolazione e analisi aggregata che sono al centro del lavoro di ogni data scientist. Inoltre, introdurremo **SciPy**, un'altra colonna portante dell'ecosistema scientifico di Python, per quantificare le relazioni tra i nostri dati.\n",
        "\n",
        "### **Obiettivi dettagliati della Lezione 7:**\n",
        "*   Svolgere e discutere l'esercizio per casa, consolidando le competenze della lezione precedente.\n",
        "*   Imparare a **gestire i dati mancanti (`NaN`)** in modo efficace.\n",
        "*   Padroneggiare una delle funzionalità più potenti di Pandas: il raggruppamento dati con **`.groupby()`**.\n",
        "*   Eseguire **analisi aggregate** per calcolare statistiche sui gruppi (es. la composizione media per tipo di roccia).\n",
        "*   Imparare a **ordinare** i dati in base ai valori con `.sort_values()`.\n",
        "*   Discutere l'importanza del **campionamento** per un'analisi visiva chiara.\n",
        "*   Quantificare i trend nei dati imparando a calcolare una **regressione lineare semplice** con **SciPy**."
      ],
      "metadata": {
        "id": "l30ttZYY5eRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parte 1: Ripasso e Preparazione dei Dati**\n",
        "\n",
        "Iniziamo la lezione svolgendo insieme l'esercizio per casa assegnato alla fine della Lezione 6. Questo ci permetterà di ripassare l'intero flusso di lavoro di base e di preparare il dataset che useremo per gli argomenti più avanzati di oggi.\n",
        "\n",
        "**Testo dell'Esercizio:**\n",
        "*   **Dataset:** \"Geochemical Variations in Igneous Rocks - Mining\" (`cristianminas/geochemical-variations-in-igneous-rocks-mining`).\n",
        "*   **Compiti:**\n",
        "    1.  Scaricare e caricare il dataset.\n",
        "    2.  Ispezionare e pulire i nomi delle colonne.\n",
        "    3.  Fare un riepilogo statisctico delle colonne numeriche.\n",
        "    4.  Filtrare i campioni di `'ANDESITE'`.\n",
        "    5.  Analizzare il contenuto di `sio2` per le andesiti.\n",
        "    6.  Creare un istogramma della distribuzione di `sio2`."
      ],
      "metadata": {
        "id": "moKvU6UJ5hzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Setup e Caricamento Dati**\n",
        "Iniziamo importando le librerie necessarie (Pandas per i dati, NumPy per il calcolo numerico, Matplotlib per i grafici) e scarichiamo il dataset direttamente da Kaggle. Una volta ottenuto il percorso del file, carichiamo il CSV in un DataFrame."
      ],
      "metadata": {
        "id": "3XenJlIwfzvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbpjwXHq4wwr"
      },
      "outputs": [],
      "source": [
        "# Importiamo tutte le librerie necessarie per l'esercizio\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"--- 1. Download e Caricamento ---\")\n",
        "# Scarichiamo il dataset\n",
        "dataset_path_igneous = kagglehub.dataset_download(\"cristianminas/geochemical-variations-in-igneous-rocks-mining\")\n",
        "# Identifichiamo il file CSV\n",
        "file_to_load_igneous = 'Data1.csv'\n",
        "full_path_igneous = os.path.join(dataset_path_igneous, file_to_load_igneous)\n",
        "# Carichiamo i dati\n",
        "df_igneous = pd.read_csv(full_path_igneous)\n",
        "print(f\"File '{file_to_load_igneous}' caricato con successo.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Pulizia dei Nomi delle Colonne**\n",
        "Spesso i dataset grezzi hanno nomi di colonne con caratteri speciali (come `*`, `(`, `)`) o formattazioni miste che rendono difficile la scrittura del codice. Qui standardizziamo i nomi rendendoli tutti minuscoli e rimuovendo i caratteri indesiderati, per poi visualizzare le prime righe."
      ],
      "metadata": {
        "id": "FdA_usNff3-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 2. Ispezione e Pulizia ---\")\n",
        "print(\"Nomi originali:\", df_igneous.columns.tolist())\n",
        "# Puliamo i nomi delle colonne (minuscolo, replace di caratteri speciali)\n",
        "df_igneous.columns = [col.lower().replace('*', '').replace('(', '').replace(')', '') for col in df_igneous.columns]\n",
        "print(\"Nomi puliti:\", df_igneous.columns.tolist())\n",
        "print(\"\\n\")\n",
        "\n",
        "display(df_igneous.head(5))"
      ],
      "metadata": {
        "id": "WeCndpgIA7hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Panoramica Statistica**\n",
        "Utilizziamo il metodo `.describe()` per ottenere un riepilogo immediato delle variabili numeriche. Questo ci permette di osservare la media, la deviazione standard, i minimi e i massimi, aiutandoci a capire il range di composizione chimica delle rocce nel dataset."
      ],
      "metadata": {
        "id": "nYxTaXdPf8Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 3. Riepilogo statistico delle colonne numeriche ---\")\n",
        "# display() formatta bene anche l'output di .describe()\n",
        "display(df_igneous.describe())"
      ],
      "metadata": {
        "id": "Ud6IyCKDA33r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Filtraggio dei Dati**\n",
        "Ora selezioniamo solo i campioni di interesse. Utilizziamo il metodo `.str.contains()` sulla colonna `rock_name` per isolare tutte le righe che contengono la parola \"ANDESITE\". L'opzione `case=False` ci assicura di non perdere dati a causa di differenze tra maiuscole e minuscole."
      ],
      "metadata": {
        "id": "Ba2-Vd7hgBPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 4. Filtraggio ---\")\n",
        "# Filtriamo per 'ANDESITE'. Usiamo .str.contains() per essere robusti a eventuali spazi extra.\n",
        "andesites = df_igneous[df_igneous['rock_name'].str.contains('ANDESITE', na=False, case=False)]\n",
        "print(f\"Trovati {len(andesites)} campioni di andesite.\\n\")"
      ],
      "metadata": {
        "id": "CmWGHZp9Aq33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Analisi Specifica**\n",
        "Ora che abbiamo isolato il DataFrame `andesites`, possiamo analizzarne le proprietà chimiche. Calcoliamo la concentrazione media di Silice (`sio2n`), che è il parametro principale per la classificazione delle rocce ignee."
      ],
      "metadata": {
        "id": "eLnYMWMbgEvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 5. Analisi ---\")\n",
        "sio2_andesites = andesites['sio2n']\n",
        "print(f\"Contenuto medio di SiO2 nelle andesiti: {sio2_andesites.mean():.2f} %\\n\")"
      ],
      "metadata": {
        "id": "d2-je-AnAoVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Visualizzazione della Distribuzione**\n",
        "Infine, creiamo un istogramma per visualizzare come varia il contenuto di silice all'interno dei campioni di andesite. Aggiungiamo una linea verticale rossa per indicare la media calcolata in precedenza, offrendo un riferimento visivo immediato rispetto alla dispersione dei dati."
      ],
      "metadata": {
        "id": "Oqn5KVJIgHCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 6. Visualizzazione ---\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Usiamo .dropna() qui per evitare che eventuali NaN nella colonna sio2n causino errori nel plot\n",
        "plt.hist(sio2_andesites.dropna(), bins=40, edgecolor='k')\n",
        "plt.title(\"Distribuzione di SiO₂ nei campioni di Andesite\")\n",
        "plt.xlabel(\"SiO₂ (%)\")\n",
        "plt.ylabel(\"Frequenza\")\n",
        "media_sio2 = sio2_andesites.mean()\n",
        "plt.axvline(media_sio2, color='red', linestyle='--', label=f'Media: {media_sio2:.2f}%')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CG7CO6FkAmnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gestire i Dati \"Sporchi\": i Valori Mancanti (`NaN`)**\n",
        "\n",
        "Come abbiamo visto ispezionando sia il dataset dei vulcani che questo nuovo dataset geochimico, è molto raro che i dati del mondo reale siano completi. Spesso, per svariati motivi (errori di misurazione, dati non raccolti, problemi di fusione di tabelle), alcune celle di una tabella sono vuote.\n",
        "\n",
        "Pandas rappresenta questi valori mancanti con un marcatore speciale, `NaN` (che sta per **\"Not a Number\"**), che eredita da NumPy.\n",
        "\n",
        "**Perché i `NaN` sono un problema?**\n",
        "La maggior parte delle operazioni matematiche che coinvolgono un `NaN` restituisce `NaN` come risultato (es. `5 + NaN = NaN`). Questo comportamento \"si propaga\" e può invalidare intere analisi statistiche o causare errori nei modelli di machine learning. È quindi essenziale identificare e gestire questi valori mancanti.\n",
        "\n",
        "**Come identificare i `NaN`**\n",
        "Abbiamo già visto il metodo `.info()` per un riepilogo. Un modo più diretto è usare `.isna()` (o il suo alias `.isnull()`):\n",
        "*   `df.isna()`: Restituisce un DataFrame delle stesse dimensioni dell'originale, ma riempito di valori booleani (`True` dove c'è un `NaN`, `False` altrimenti).\n",
        "*   `df.isna().sum()`: Un comando potentissimo. Concatena `.sum()` al metodo precedente per contare il numero di `True` (e quindi di `NaN`) in ogni colonna.\n"
      ],
      "metadata": {
        "id": "94RyNsP_5w0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useremo il DataFrame 'df_igneous' caricato nell'esercizio precedente.\n",
        "\n",
        "print(\"--- Identificazione dei Valori Mancanti ---\")\n",
        "# Contiamo quanti NaN ci sono in ogni colonna\n",
        "missing_values = df_igneous.isna().sum()\n",
        "\n",
        "print(\"Numero di valori mancanti per colonna:\")\n",
        "# Mostriamo solo le colonne che hanno effettivamente valori mancanti\n",
        "print(missing_values[missing_values > 0])"
      ],
      "metadata": {
        "id": "u9duMpQm53A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Gestione dei Valori Mancanti con .dropna() ---\n",
        "print(\"\\n--- Rimozione dei Valori Mancanti ---\")\n",
        "print(f\"Numero di righe originali nel DataFrame: {len(df_igneous)}\")"
      ],
      "metadata": {
        "id": "kMwerOnqqwH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Come gestire i `NaN`**\n",
        "Esistono strategie complesse per modificare i valori mancanti (es. sostituirli con la media), ma la strategia più semplice e diretta è **eliminarli**.\n",
        "*   **`.dropna()`**: Questo metodo rimuove le righe (o colonne) che contengono valori `NaN`.\n",
        "    *   **Argomento `subset`**: Per default, `.dropna()` elimina una riga se contiene *anche un solo* `NaN` in una qualsiasi colonna. Questo può essere troppo drastico. Possiamo usare `subset=['colonna1', 'colonna2']` per specificare che vogliamo eliminare una riga solo se il `NaN` è presente in una di quelle colonne specifiche, che sono cruciali per la nostra analisi."
      ],
      "metadata": {
        "id": "UNtyiMDNrUpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Per la nostra analisi, le colonne degli ossidi principali sono fondamentali.\n",
        "# Creiamo un nuovo DataFrame 'df_clean' eliminando solo le righe\n",
        "# che hanno NaN nelle colonne di SiO2, MgO o rock_name.\n",
        "colonne_chiave = ['sio2n', 'mgon', 'rock_name']\n",
        "#df_clean = df_igneous.dropna(subset=colonne_chiave)\n",
        "df_clean = df_igneous.dropna(subset=colonne_chiave).copy()\n",
        "\n",
        "print(f\"Numero di righe dopo aver rimosso i NaN dalle colonne chiave: {len(df_clean)}\")\n"
      ],
      "metadata": {
        "id": "_oBzJXDJqrBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifichiamo che non ci siano più NaN in quelle colonne specifiche\n",
        "print(\"\\nVerifica dei valori mancanti nel DataFrame pulito (colonne chiave):\")\n",
        "print(df_clean[colonne_chiave].isna().sum())"
      ],
      "metadata": {
        "id": "trrhlzTKql80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nota per la lezione: d'ora in poi, per le nostre analisi, useremo questo\n",
        "# DataFrame pulito, 'df_clean', per garantire la correttezza dei calcoli.\n",
        "\n",
        "# Usiamo di nuovo .info() sul nostro DataFrame pulito per avere una visione chiara\n",
        "# dei tipi di dato di ogni colonna.\n",
        "print(\"Ispezione dei tipi di dato in 'df_clean':\")\n",
        "df_clean.info()"
      ],
      "metadata": {
        "id": "-pvkIuwiqjuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Investigare i Dati \"Sporchi\": Come Trovare i Valori Non Numerici**\n",
        "\n",
        "L'output di `.info()` ci ha mostrato che la colonna `p2o5n` è di tipo `object`, il che indica la presenza di testo. Ma come facciamo a trovare esattamente quali sono le righe e i valori problematici in un DataFrame che potrebbe avere migliaia di righe?\n",
        "\n",
        "Possiamo usare una tecnica di \"diagnosi\" molto potente:\n",
        "\n",
        "1.  Useremo `pd.to_numeric(df['p2o5n'], errors='coerce')` per creare una **copia temporanea** della colonna in formato numerico. L'opzione `errors='coerce'` trasformerà tutti i valori che non sono numeri in `NaN`.\n",
        "2.  Creeremo una maschera booleana per trovare dove sono questi `NaN` nella copia temporanea, usando il metodo `.isna()`.\n",
        "3.  Useremo questa maschera per filtrare il **DataFrame originale** e visualizzare solo le righe che contengono i dati \"sporchi\"."
      ],
      "metadata": {
        "id": "LUhAxV2kWo-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assicuriamoci che df_clean esista\n",
        "if 'df_clean' in locals() and not df_clean.empty:\n",
        "\n",
        "    # 1. Creiamo una copia temporanea della colonna in formato numerico (con NaN per gli errori)\n",
        "    p2o5n_numeric_temp = pd.to_numeric(df_clean['p2o5n'], errors='coerce')\n",
        "\n",
        "    # 2. Creiamo una maschera booleana che è True dove la conversione ha fallito (producendo NaN)\n",
        "    #    e dove il valore originale non era già NaN (per isolare solo i nuovi problemi).\n",
        "    valori_problematici_mask = p2o5n_numeric_temp.isna() & df_clean['p2o5n'].notna()\n",
        "\n",
        "    # 3. Usiamo la maschera per visualizzare le righe problematiche nel DataFrame originale\n",
        "    righe_problematiche = df_clean[valori_problematici_mask]\n",
        "\n",
        "    if not righe_problematiche.empty:\n",
        "        print(f\"Trovate {len(righe_problematiche)} righe con valori non numerici nella colonna 'p2o5n':\")\n",
        "        # Mostriamo le righe e in particolare la colonna 'p2o5n'\n",
        "        display(righe_problematiche[['rock_name', 'sio2n', 'p2o5n']])\n",
        "    else:\n",
        "        print(\"Nessun valore non numerico trovato (potrebbero essere già NaN).\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame 'df_clean' non trovato.\")"
      ],
      "metadata": {
        "id": "H_LJFB_uWrVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Correzione della Colonna Problematica**\n",
        "\n",
        "La nostra investigazione ha avuto successo: abbiamo identificato che il valore non numerico `'<0.05'` è la causa del tipo `object` della colonna `p2o5n`. Questa notazione, che probabilmente indica un valore sotto il limite di detezione, deve essere gestita prima di poter procedere con i calcoli.\n",
        "\n",
        "La strategia più sicura e comune in questi casi è trattare questi valori come dati mancanti, in quanto non conosciamo il valore esatto.\n",
        "\n",
        "Applicheremo ora la conversione in modo permanente al nostro DataFrame `df_clean`, usando `pd.to_numeric` con `errors='coerce'`. Questo comando sostituirà tutti i valori come `'<0.05'` con `NaN` (Not a Number), il marcatore standard di Pandas per i dati mancanti, rendendo la colonna puramente numerica."
      ],
      "metadata": {
        "id": "n_p6RUktXVSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Controlliamo se df_clean esiste prima di modificarlo\n",
        "if 'df_clean' in locals() and not df_clean.empty:\n",
        "    print(f\"Tipo di dato di 'p2o5n' PRIMA della correzione: {df_clean['p2o5n'].dtype}\")\n",
        "\n",
        "    # Applichiamo la conversione in modo permanente alla colonna del nostro DataFrame\n",
        "    # L'uso di .loc[:, 'p2o5n'] è il modo più corretto per assegnare i valori\n",
        "    # ed evitare il SettingWithCopyWarning che abbiamo discusso.\n",
        "\n",
        "    # df_clean = df_clean.copy()\n",
        "    df_clean['p2o5n'] = pd.to_numeric(df_clean['p2o5n'], errors='coerce')\n",
        "\n",
        "    print(f\"Tipo di dato di 'p2o5n' DOPO la correzione: {df_clean['p2o5n'].dtype}\")\n",
        "\n",
        "    # Verifichiamo il risultato finale con .info()\n",
        "    # Noteremo che 'p2o5n' è ora float64 e il suo conteggio di valori non-nulli è diminuito.\n",
        "    print(\"\\nNuovo riepilogo del DataFrame 'df_clean':\")\n",
        "    df_clean.info()\n",
        "else:\n",
        "    print(\"DataFrame 'df_clean' non trovato o vuoto.\")"
      ],
      "metadata": {
        "id": "XObqXSNwXX2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parte 2: Raggruppamento e Analisi Aggregata**\n",
        "\n",
        "Finora abbiamo filtrato i dati per selezionare sottoinsiemi di righe. Ma cosa succede se vogliamo eseguire un calcolo *su ogni categoria* presente nei nostri dati? Ad esempio, \"calcola la composizione chimica media per *ogni* tipo di roccia\" invece di farlo solo per le andesiti.\n",
        "\n",
        "Eseguire questa operazione con filtri separati per ogni tipo di roccia sarebbe estremamente ripetitivo e inefficiente. Per questo tipo di analisi aggregata, Pandas offre una funzionalità potentissima: il metodo **`.groupby()`**.\n",
        "\n",
        "### **La Logica \"Split-Apply-Combine\"**\n",
        "\n",
        "Il funzionamento di `.groupby()` si basa su un processo in tre fasi, noto come **\"Split-Apply-Combine\"** (Dividi-Applica-Combina):\n",
        "\n",
        "1.  **Split (Dividi):** Il DataFrame originale viene suddiviso virtualmente in gruppi, basandosi sui valori unici di una o più colonne. Ad esempio, `df.groupby('rock_name')` creerà un gruppo per 'BASALT', uno per 'ANDESITE', uno per 'RHYOLITE', e così via.\n",
        "\n",
        "2.  **Apply (Applica):** Una funzione di aggregazione (come `.mean()`, `.sum()`, `.count()`, `.std()`) viene applicata in modo indipendente a ogni singolo gruppo. Per esempio, `.mean()` calcolerà la media di tutte le colonne numeriche per i basalti, poi per le andesiti, ecc.\n",
        "\n",
        "3.  **Combine (Combina):** I risultati di ogni applicazione vengono raccolti e combinati in un nuovo DataFrame, dove l'indice è costituito dalle categorie usate per il raggruppamento.\n",
        "\n",
        "Questa operazione ci permette di passare da una tabella di dati grezzi a una tabella di statistiche riassuntive con una sola, leggibile riga di codice."
      ],
      "metadata": {
        "id": "88J8fxOQ6Hpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useremo il nostro DataFrame pulito, 'df_clean', per assicurarci\n",
        "# che i calcoli delle medie non siano influenzati da valori mancanti.\n",
        "\n",
        "print(\"--- Calcolo della composizione media per tipo di roccia ---\")\n",
        "\n",
        "# 1. SPLIT: Raggruppiamo il DataFrame per la colonna 'rock_name'.\n",
        "# 2. APPLY: Calcoliamo la media (.mean()) per ogni gruppo.\n",
        "# 3. COMBINE: Pandas restituisce un nuovo DataFrame con i risultati.\n",
        "#    Di default, il calcolo viene eseguito solo sulle colonne numeriche.\n",
        "mean_composition = df_clean.groupby('rock_name').mean()\n",
        "\n",
        "# Visualizziamo le prime 10 righe del risultato\n",
        "print(\"Composizione media calcolata per ogni tipo di roccia (prime 10):\")\n",
        "display(mean_composition.head(10))"
      ],
      "metadata": {
        "id": "w_-6lzxt6KYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problema Comune: Categorie Duplicate a Causa di Spazi Nascosti**\n",
        "\n",
        "Dopo aver eseguito un `groupby`, potremmo notare risultati inaspettati, come la stessa categoria (es. 'Andesite') che appare più volte. Questo è quasi sempre un sintomo di \"dati sporchi\" all'interno della colonna usata per il raggruppamento.\n",
        "\n",
        "La causa più comune sono gli **spazi bianchi extra** all'inizio o alla fine delle stringhe. Per Pandas, `'Andesite'` e `'Andesite '` sono due categorie distinte.\n",
        "\n",
        "Per verificare questo problema, possiamo usare il metodo `.unique()` su una colonna per vedere tutti i suoi valori unici. Se troviamo varianti dello stesso nome, abbiamo confermato il problema."
      ],
      "metadata": {
        "id": "sFX9o5NsbRjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ispezioniamo i valori unici nella colonna 'rock_name' del nostro DataFrame pulito\n",
        "if 'df_clean' in locals() and not df_clean.empty:\n",
        "\n",
        "    print(\"Valori unici nella colonna 'rock_name':\")\n",
        "    unique_rocks = df_clean['rock_name'].unique()\n",
        "\n",
        "    # Stampiamo tutti i valori unici per ispezionarli visivamente\n",
        "    print(unique_rocks)\n",
        "\n",
        "    # Un modo programmatico per trovare problemi:\n",
        "    # Cerchiamo i nomi delle rocce che contengono 'ANDESITE'\n",
        "    andesite_variations = [rock for rock in unique_rocks if 'ANDESITE' in str(rock)]\n",
        "    print(\"\\nVariazioni trovate per 'ANDESITE':\", andesite_variations)\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame 'df_clean' non trovato.\")"
      ],
      "metadata": {
        "id": "roUK4CyKbURa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dall'output, notiamo due problemi distinti:\n",
        "\n",
        "1.  **Spazi Extra:** Alcuni nomi hanno spazi alla fine (es. vedremo `'Andesite '` o `'Gabbro  '`). Questi vengono trattati come categorie separate da `'Andesite'` e `'Gabbro'`.\n",
        "2.  **Eccessiva Granularità:** Ci sono molte sottocategorie (es. `'Andesite dike'`, `'Andesite tuff'`). Per un'analisi aggregata generale, potremmo volerle raggruppare tutte sotto un'unica etichetta, \"Andesite\".\n",
        "\n",
        "Risolviamo questi problemi in due passaggi.\n",
        "\n"
      ],
      "metadata": {
        "id": "9rVVmBbzdnbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Passo 1: Pulizia degli Spazi con `.str.strip()`**\n",
        "\n",
        "Come prima cosa, applichiamo il metodo `.str.strip()` per rimuovere tutti gli spazi bianchi all'inizio e alla fine di ogni nome di roccia. Questo accorperà le categorie che sembravano diverse solo a causa di errori di formattazione (es. `'Andesite '` diventerà uguale a `'Andesite'`).\n",
        "\n",
        "Per misurare l'efficacia di questa operazione, useremo il metodo **`.nunique()`** (abbreviazione di *Number of Unique*).\n",
        "È importante distinguere due metodi simili:\n",
        "*   **`.unique()`**: Restituisce **l'elenco** dei valori distinti (es. `['Basalto', 'Andesite']`).\n",
        "*   **`.nunique()`**: Restituisce **il numero** totale di valori distinti (es. `2`).\n",
        "\n",
        "Confrontando il conteggio (`nunique`) prima e dopo la pulizia, sapremo esattamente quante categorie duplicate abbiamo eliminato."
      ],
      "metadata": {
        "id": "dCh5I1jqui51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_clean' in locals():\n",
        "    print(f\"Nomi unici prima di .strip(): {df_clean['rock_name'].nunique()}\")\n",
        "\n",
        "    # Usiamo .loc[:, 'rock_name'] per dire a Pandas:\n",
        "    # \"Prendi TUTTE le righe (:) e aggiorna la colonna 'rock_name'\"\n",
        "    df_clean.loc[:, 'rock_name'] = df_clean['rock_name'].str.strip()\n",
        "\n",
        "    print(f\"Nomi unici dopo .strip(): {df_clean['rock_name'].nunique()}\")\n",
        "    # Il numero di nomi unici dovrebbe essere diminuito."
      ],
      "metadata": {
        "id": "atR-6ee4dpxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Passo 2: Creare una Categoria Semplificata**\n",
        "\n",
        "Ora che abbiamo pulito gli spazi, affrontiamo il problema della granularità. Abbiamo troppi nomi specifici (es. \"High-K Basalt\", \"Alkali Basalt\") e vogliamo raggrupparli in macro-famiglie (es. \"Basaltic\").\n",
        "\n",
        "Per fare questo, useremo un costrutto molto potente e frequente in Pandas, che permette di fare una **assegnazione condizionale**.\n",
        "\n",
        "La sintassi è:\n",
        "`df.loc[CONDIZIONE_RIGHE, COLONNA_DA_MODIFICARE] = NUOVO_VALORE`\n",
        "\n",
        "Analizziamo il comando che useremo per i Basalti:\n",
        "```python\n",
        "df_clean.loc[df_clean['rock_name'].str.contains('Basalt', case=False, na=False), 'rock_category'] = 'Basaltic'\n",
        "```\n",
        "\n",
        "Ecco cosa fa ogni parte:\n",
        "1.  **`.loc[...]`**: È il modo corretto per selezionare dati specifici quando vogliamo modificarli (evita avvisi di memoria). Accetta due argomenti: *quali righe* e *quale colonna*.\n",
        "2.  **`df_clean['rock_name'].str.contains(...)`**: Questa è la **condizione per le righe**. Cerca la sottostringa \"Basalt\" dentro ogni nome.\n",
        "    *   **`case=False`**: Ignora le maiuscole/minuscole (trova sia \"Basalt\" che \"basalt\").\n",
        "    *   **`na=False`**: Se incontra un valore mancante, lo considera `False` invece di dare errore.\n",
        "3.  **`'rock_category'`**: È la **colonna** dove vogliamo scrivere il risultato.\n",
        "4.  **`= 'Basaltic'`**: È il **valore** che assegniamo a quelle specifiche celle.\n",
        "\n",
        "In parole povere: *\"Cerca tutte le righe dove il nome contiene 'Basalt' e, nella colonna 'rock_category' di quelle righe, scrivi 'Basaltic'.\"*"
      ],
      "metadata": {
        "id": "HWN3f9AzvTwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_clean' in locals():\n",
        "    # Creiamo una nuova colonna 'rock_category' inizializzata con 'Other'\n",
        "    df_clean.loc[:, 'rock_category'] = 'Other'\n",
        "\n",
        "    # Usiamo .loc e .str.contains() per assegnare le categorie principali\n",
        "    # case=False rende la ricerca insensibile a maiuscole/minuscole\n",
        "    df_clean.loc[df_clean['rock_name'].str.contains('Basalt', case=False, na=False), 'rock_category'] = 'Basaltic'\n",
        "    df_clean.loc[df_clean['rock_name'].str.contains('Andesite', case=False, na=False), 'rock_category'] = 'Andesitic'\n",
        "    df_clean.loc[df_clean['rock_name'].str.contains('Dacite', case=False, na=False), 'rock_category'] = 'Dacitic'\n",
        "    df_clean.loc[df_clean['rock_name'].str.contains('Rhyolite', case=False, na=False), 'rock_category'] = 'Rhyolitic'\n",
        "    df_clean.loc[df_clean['rock_name'].str.contains('Granite', case=False, na=False), 'rock_category'] = 'Granitic'\n",
        "\n",
        "    # Ora controlliamo il risultato con .value_counts()\n",
        "    print(\"Conteggio delle nuove categorie semplificate:\")\n",
        "    display(df_clean['rock_category'].value_counts())"
      ],
      "metadata": {
        "id": "1LdDpOzedy1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Eseguire l'Analisi Aggregata sui Dati Puliti**\n",
        "Ora che abbiamo una colonna `'rock_category'` pulita e semplificata, possiamo finalmente eseguire il nostro `groupby` per calcolare la composizione media di ogni categoria principale."
      ],
      "metadata": {
        "id": "RdwPkWuGd013"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_clean' in locals():\n",
        "    # Eseguiamo il groupby sulla nuova colonna 'rock_category'\n",
        "    mean_composition_simplified = df_clean.groupby('rock_category').mean(numeric_only=True)\n",
        "\n",
        "    print(\"Composizione media per categoria di roccia semplificata:\")\n",
        "    display(mean_composition_simplified[['sio2n', 'mgon', 'k2on']]) # Mostriamo solo alcune colonne"
      ],
      "metadata": {
        "id": "8nEzaKHrd2qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ordinare i Dati con `.sort_values()`**\n",
        "\n",
        "Il risultato del nostro `groupby` è un DataFrame, e come ogni DataFrame, possiamo ordinarlo per analizzarlo meglio. Il metodo `.sort_values()` ci permette di ordinare le righe in base ai valori di una o più colonne.\n",
        "\n",
        "Gli argomenti più importanti sono:\n",
        "*   `by`: il nome della colonna (o una lista di nomi) su cui basare l'ordinamento.\n",
        "*   `ascending`: `True` per un ordine crescente (default), `False` per un ordine decrescente.\n",
        "\n",
        "Ad esempio, ordiniamo i risultati precedenti per vedere quali tipi di roccia hanno, in media, il più alto contenuto di Silice."
      ],
      "metadata": {
        "id": "tKZCUFjH6Mi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordiniamo il DataFrame 'mean_composition' in base alla colonna 'sio2n'\n",
        "# in ordine decrescente (dal più alto al più basso).\n",
        "sorted_composition = mean_composition_simplified.sort_values(by='sio2n', ascending=False)\n",
        "\n",
        "print(\"Tipi di roccia ordinati per contenuto medio di SiO₂ (primi 10):\")\n",
        "# Mostriamo le prime 10 rocce più ricche in silice\n",
        "display(sorted_composition.head(10))\n",
        "\n",
        "print(\"\\nTipi di roccia ordinati per contenuto medio di MgO (primi 10):\")\n",
        "# E ora per MgO (ci aspettiamo un ordine inverso)\n",
        "display(mean_composition_simplified.sort_values(by='mgon', ascending=False).head(10))"
      ],
      "metadata": {
        "id": "B_GMUTzM6Og6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Il Campionamento dei Dati (`.sample()`)**\n",
        "\n",
        "Il nostro dataset contiene migliaia di righe. Quando si lavora con dataset molto grandi (milioni di righe), visualizzare o elaborare tutti i dati può essere lento e portare a grafici illeggibili per la troppa sovrapposizione di punti.\n",
        "\n",
        "Spesso è utile lavorare su un **sottoinsieme** (campione) dei dati. Pandas offre il metodo `.sample()` per estrarre righe a caso.\n",
        "Proviamo a estrarre un campione casuale di 500 rocce e vediamo come sono distribuite le categorie."
      ],
      "metadata": {
        "id": "qVx21IUwjvs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impostiamo un random_state per ottenere sempre gli stessi risultati (riproducibilità)\n",
        "print(\"--- Campionamento Casuale Semplice (500 campioni) ---\")\n",
        "simple_sample = df_clean.sample(n=500, random_state=42)\n",
        "\n",
        "# Verifichiamo la distribuzione delle categorie in questo campione\n",
        "print(\"Conteggio per categoria nel campione casuale:\")\n",
        "counts_simple = simple_sample['rock_category'].value_counts()\n",
        "display(counts_simple)\n",
        "\n",
        "# Calcoliamo la percentuale rispetto al totale del campione\n",
        "print(\"\\nPercentuali nel campione casuale:\")\n",
        "display(counts_simple / len(simple_sample) * 100)"
      ],
      "metadata": {
        "id": "d8u8BcK0jy5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Il Problema del Bilanciamento (Bias)**\n",
        "\n",
        "Osservando i numeri qui sopra, noterete che le categorie più comuni nel dataset originale (come 'Basaltic' o 'Andesitic') dominano anche il campione. Le categorie più rare (come 'Rhyolitic' o 'Granitic') potrebbero avere pochissimi rappresentanti o addirittura scomparire nel campione.\n",
        "\n",
        "Se vogliamo confrontare le caratteristiche chimiche dei vari gruppi, questo è un problema: non possiamo calcolare statistiche affidabili su un gruppo se abbiamo solo 3 o 4 campioni!\n",
        "\n",
        "Per risolvere questo problema, dobbiamo effettuare un **Campionamento Bilanciato** (o Stratificato). Invece di pescare a caso dal mucchio totale, pescheremo un numero fisso di campioni *da ogni gruppo*.\n",
        "\n",
        "Useremo:\n",
        "1.  `.groupby('rock_category')`: Per dividere i dati in gruppi.\n",
        "2.  `.apply(...)`: Per applicare un campionamento su ogni singolo gruppo."
      ],
      "metadata": {
        "id": "aBUhTxmOwAAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Campionamento Bilanciato (Stratificato) ---\")\n",
        "\n",
        "# Definiamo quanti campioni vogliamo per ogni categoria\n",
        "n_samples_per_category = 120\n",
        "\n",
        "# Invece di usare .apply(), usiamo un approccio più esplicito e sicuro:\n",
        "# 1. Creiamo una lista vuota per raccogliere i campioni\n",
        "sampled_dfs = []\n",
        "\n",
        "# 2. Cicliamo su ogni gruppo creato dal groupby\n",
        "for category_name, group_df in df_clean.groupby('rock_category'):\n",
        "    # Calcoliamo n (prendiamo 50 oppure tutti se sono meno di 50)\n",
        "    n = min(len(group_df), n_samples_per_category)\n",
        "\n",
        "    # Campioniamo e aggiungiamo il risultato alla lista\n",
        "    sampled_dfs.append(group_df.sample(n=n, random_state=42))\n",
        "\n",
        "# 3. Concateniamo tutti i pezzetti in un unico DataFrame finale\n",
        "balanced_sample = pd.concat(sampled_dfs)\n",
        "\n",
        "# Verifichiamo la nuova distribuzione\n",
        "print(\"Conteggio per categoria nel campione bilanciato:\")\n",
        "display(balanced_sample['rock_category'].value_counts())\n",
        "\n",
        "print(f\"\\nDimensione totale campione bilanciato: {len(balanced_sample)}\")"
      ],
      "metadata": {
        "id": "iGJLkPOAj3fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parte 3: Analisi di Correlazione e Regressione Lineare**\n",
        "\n",
        "Finora abbiamo analizzato le colonne singolarmente o abbiamo aggregato i dati per categoria. Un altro compito fondamentale dell'analisi dati è capire la **relazione tra due variabili numeriche**.\n",
        "\n",
        "### **Visualizzare le Correlazioni: i Diagrammi di Harker**\n",
        "\n",
        "Come abbiamo visto nelle lezioni precedenti, il grafico a dispersione (`scatter plot`) è lo strumento ideale per questo. In geochimica, un'applicazione classica sono i **diagrammi di Harker**, che plottano la concentrazione di vari ossidi contro quella della silice (SiO₂).\n",
        "\n",
        "Questi diagrammi sono potentissimi perché ci permettono di visualizzare i **trend di differenziazione magmatica**. Ad esempio, ci aspettiamo che mentre il contenuto di silice aumenta (rocce più \"evolute\" o felsiche), il contenuto di ossidi come il magnesio (MgO) diminuisca.\n",
        "\n",
        "Visualizziamo questo trend con i nostri dati."
      ],
      "metadata": {
        "id": "H__CfMhS6e65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useremo il DataFrame pulito 'df_clean'.\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Creiamo un grafico a dispersione di SiO2 vs MgO\n",
        "# Usiamo alpha=0.3 per gestire la sovrapposizione dei punti\n",
        "plt.scatter(df_clean['sio2n'], df_clean['mgon'], alpha=0.3)\n",
        "\n",
        "plt.title(\"Diagramma di Harker: MgO vs SiO₂\")\n",
        "plt.xlabel(\"SiO₂ (%)\")\n",
        "plt.ylabel(\"MgO (%)\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Come previsto, osserviamo una chiara correlazione negativa."
      ],
      "metadata": {
        "id": "67AUnYdT6klz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizzazione Avanzata: Creare un Diagramma di Harker Categoriale**\n",
        "\n",
        "Nella lezione precedente abbiamo visto come creare scatter plot e come usare colori diversi per distinguere più serie di dati. Ora possiamo combinare questa tecnica con la nostra analisi di Pandas per creare una visualizzazione molto più ricca.\n",
        "\n",
        "Invece di plottare tutti i punti con lo stesso colore, possiamo colorare ogni punto in base alla categoria di roccia a cui appartiene (la colonna `'rock_category'` che abbiamo appena creato). Questo ci permetterà di vedere se le diverse famiglie di rocce (basaltiche, andesitiche, ecc.) si raggruppano in zone specifiche del diagramma.\n",
        "\n",
        "**La Strategia:**\n",
        "Il modo più elegante per fare questo con Matplotlib è:\n",
        "1.  Ottenere la lista di tutte le categorie uniche di rocce.\n",
        "2.  Creare un ciclo `for` che itera su ogni categoria.\n",
        "3.  All'interno del ciclo:\n",
        "    a. Selezionare un sotto-DataFrame contenente solo le rocce di quella categoria.\n",
        "    b. Creare uno `scatter plot` solo per quel sotto-DataFrame, assegnandogli un colore e un'etichetta specifici.\n",
        "\n",
        "In questo modo, Matplotlib creerà automaticamente un grafico con punti di colori diversi e una legenda corretta."
      ],
      "metadata": {
        "id": "8lfSn1jUipfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assicuriamoci che df_clean e la colonna 'rock_category' esistano\n",
        "if 'balanced_sample' in locals() and 'rock_category' in balanced_sample.columns:\n",
        "\n",
        "    # --- Creazione di un Diagramma di Harker Categoriale ---\n",
        "\n",
        "    # 1. Definiamo i dati da plottare\n",
        "    x_axis = 'sio2n'\n",
        "    y_axis = 'mgon'\n",
        "\n",
        "    # Inizializziamo la figura\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # 2. Otteniamo la lista delle categorie uniche di rocce\n",
        "    categories = balanced_sample['rock_category'].unique()\n",
        "\n",
        "    # (Opzionale) Creiamo una mappa di colori per assegnare un colore a ogni categoria\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(categories)))\n",
        "    color_map = dict(zip(categories, colors))\n",
        "\n",
        "    # 3. Cicliamo su ogni categoria per plottarla separatamente\n",
        "    for category in categories:\n",
        "        # Selezioniamo il sotto-DataFrame per la categoria corrente\n",
        "        subset = balanced_sample[balanced_sample['rock_category'] == category]\n",
        "\n",
        "        # Plottiamo solo i dati di questo subset\n",
        "        plt.scatter(subset[x_axis], subset[y_axis],\n",
        "                    label=category,\n",
        "                    color=color_map[category],\n",
        "                    alpha=0.6, s=50) # s=50 per la dimensione dei punti\n",
        "\n",
        "    # Personalizziamo il grafico\n",
        "    plt.title(f\"Diagramma di Harker: {y_axis.upper()} vs {x_axis.upper()}\")\n",
        "    plt.xlabel(\"SiO₂ (%)\")\n",
        "    plt.ylabel(\"MgO (%)\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.25)\n",
        "\n",
        "    # Aggiungiamo la legenda\n",
        "    plt.legend(title=\"Categoria di Roccia\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame 'df_clean' o colonna 'rock_category' non trovati. Esegui le celle precedenti.\")"
      ],
      "metadata": {
        "id": "YmtSW0XwirZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quantificare il Trend: Regressione Lineare Semplice con SciPy**\n",
        "\n",
        "Il nostro occhio vede chiaramente una linea. Ma come possiamo descrivere questa relazione matematicamente? Vogliamo trovare l'equazione della retta (`y = mx + q`) che \"meglio approssima\" i nostri dati. Questo processo è chiamato **regressione lineare semplice**.\n",
        "\n",
        "Per eseguirla, useremo la libreria **SciPy** (Scientific Python), un'altra colonna portante dell'ecosistema scientifico di Python che fornisce algoritmi per l'ottimizzazione, l'algebra lineare, l'integrazione e, appunto, la statistica.\n",
        "\n",
        "La funzione che ci interessa è `linregress()` dal modulo `scipy.stats`.\n",
        "\n",
        "**La funzione `linregress(x, y)`:**\n",
        "*   Prende in input due sequenze di dati (le nostre colonne `x` e `y`).\n",
        "*   Esegue i calcoli della regressione lineare.\n",
        "*   Restituisce una tupla con cinque valori importantissimi:\n",
        "    *   `slope` (pendenza): il coefficiente `m` della retta. Ci dice di quanto cambia `y` per ogni unità di `x`.\n",
        "    *   `intercept` (intercetta): il valore `q` della retta. È il valore di `y` quando `x` è 0.\n",
        "    *   `rvalue` (coefficiente di correlazione): un valore tra -1 e 1 che misura la forza e la direzione della relazione lineare.\n",
        "    *   `pvalue`: un valore statistico che ci aiuta a capire se la correlazione trovata è significativa o dovuta al caso (un p-value basso, es. < 0.05, è buono).\n",
        "    *   `stderr`: l'errore standard della stima della pendenza.\n",
        "\n",
        "Nella prossima cella, useremo questa funzione per calcolare la retta di miglior fit e la sovrapporremo al nostro grafico."
      ],
      "metadata": {
        "id": "BPxtFda16njP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importiamo la funzione linregress dal modulo scipy.stats\n",
        "from scipy.stats import linregress\n",
        "\n",
        "# Assicuriamoci che il DataFrame 'df_clean' esista e non sia vuoto\n",
        "if 'df_clean' in locals() and not df_clean.empty:\n",
        "\n",
        "    # --- Esecuzione della Regressione Lineare ---\n",
        "\n",
        "    # 1. Prepariamo i dati\n",
        "    # Per evitare problemi con i NaN che potrebbero essere ancora presenti,\n",
        "    # creiamo un piccolo DataFrame temporaneo con solo le due colonne di interesse\n",
        "    # e rimuoviamo le righe dove almeno uno dei due valori è mancante.\n",
        "    regression_data = df_clean[['sio2n', 'mgon']].dropna()\n",
        "\n",
        "    x_data = regression_data['sio2n']\n",
        "    y_data = regression_data['mgon']\n",
        "\n",
        "    # 2. Eseguiamo la regressione lineare con linregress\n",
        "    slope, intercept, r_value, p_value, std_err = linregress(x_data, y_data)\n",
        "\n",
        "    # 3. Stampiamo i risultati in modo leggibile\n",
        "    print(\"--- Risultati della Regressione Lineare (MgO vs SiO₂) ---\")\n",
        "    print(f\"Pendenza (slope, m): {slope:.4f}\")\n",
        "    print(f\"Intercetta (intercept, q): {intercept:.4f}\")\n",
        "    print(f\"Coefficiente di correlazione (R): {r_value:.4f}\")\n",
        "    print(f\"Coefficiente di determinazione (R-squared): {r_value**2:.4f}\")\n",
        "    print(f\"P-value: {p_value:.3e}\") # Usiamo la notazione scientifica per p-value molto piccoli\n",
        "\n",
        "    # Interpretazione dei risultati\n",
        "    print(\"\\nInterpretazione:\")\n",
        "    print(f\"La retta di regressione è: MgO = {slope:.2f} * SiO₂ + {intercept:.2f}\")\n",
        "    if p_value < 0.05:\n",
        "        print(\"Il p-value è molto basso, indicando che la correlazione è statisticamente significativa.\")\n",
        "    if r_value < 0:\n",
        "        print(f\"Il coefficiente di correlazione R di {r_value:.2f} indica una forte correlazione negativa.\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame 'df_clean' non trovato. Impossibile eseguire la regressione.\")"
      ],
      "metadata": {
        "id": "MMyjuMKpkG8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretazione dei Risultati Statistici\n",
        "\n",
        "Vediamo ora come interpretare i parametri statistici chiave ottenuti dalla regressione lineare tra **MgO** (variabile dipendente) e **SiO₂** (variabile indipendente).\n",
        "\n",
        "#### 1. Coefficiente di determinazione ($R^2$)\n",
        "Il **Coefficiente di determinazione**, o $R^2$, misura la proporzione della varianza nella variabile dipendente che è prevedibile dalla variabile indipendente. Varia da 0 a 1 e rappresenta la \"bontà di adattamento\" del modello ai dati reali.\n",
        "*   **Significato generale:** Un valore vicino a 1 indica che il modello spiega bene la variabilità dei dati; un valore vicino a 0 indica che il modello non spiega la variabilità.\n",
        "*   **Nel nostro caso:** Il valore ottenuto di **0.6932** indica che circa il **69.3%** della variazione nel contenuto di MgO può essere spiegata dalla variazione del contenuto di SiO₂ attraverso questo modello lineare. Il restante ~30.7% della variabilità è dovuto ad altri fattori non inclusi nel modello o a varianza casuale.\n",
        "\n",
        "#### 2. P-value\n",
        "Il **P-value** viene utilizzato per verificare la significatività statistica della relazione ipotizzata. Testa l'ipotesi nulla ($H_0$) secondo cui non esiste alcuna relazione tra le due variabili (ovvero, che la pendenza della retta sia zero).\n",
        "*   **Significato generale:** Solitamente, un P-value inferiore a **0.05** (5%) indica che si può rifiutare l'ipotesi nulla, confermando che esiste una relazione significativa tra le variabili.\n",
        "*   **Nel nostro caso:** Il P-value di **0.000e+00** (praticamente zero) è ben al di sotto della soglia di significatività di 0.05. Ciò ci permette di affermare con altissima confidenza che la relazione osservata tra MgO e SiO₂ **è statisticamente significativa** e non è frutto del caso.\n",
        "\n",
        "---\n",
        "**Conclusione:**\n",
        "I dati mostrano una relazione **significativa** (basso P-value) e **moderatamente forte** (buon $R^2$). Dato che la pendenza ($m$) e il coefficiente di correlazione ($R$) sono negativi, possiamo confermare che esiste una **correlazione inversa**: all'aumentare della silice ($SiO_2$), il contenuto di magnesio ($MgO$) diminuisce sistematicamente."
      ],
      "metadata": {
        "id": "Oi6_F_mKSgyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizzare la Retta di Regressione**\n",
        "\n",
        "Abbiamo calcolato i parametri della retta che meglio approssima i nostri dati (`slope` e `intercept`). Ora, il passo finale è visualizzare questa retta direttamente sopra i nostri dati per valutarne visivamente la bontà dell'adattamento (*goodness of fit*).\n",
        "\n",
        "Per disegnare una linea, Matplotlib ha bisogno solo di due punti. La nostra strategia sarà:\n",
        "\n",
        "1.  **Generare i Punti per la Retta:**\n",
        "    *   Creeremo un piccolo array NumPy `x_line` contenente solo due valori: il minimo e il massimo del nostro range di SiO₂.\n",
        "    *   Applicheremo l'equazione della retta (`y = mx + q`) a questi due punti per calcolare i corrispondenti valori `y_line`. Questi due punti `(x_min, y_min_calcolato)` e `(x_max, y_max_calcolato)` definiscono l'inizio e la fine della nostra retta di regressione.\n",
        "\n",
        "2.  **Creare il Grafico:**\n",
        "    *   Per prima cosa, ricreeremo lo `scatter plot` originale con tutti i nostri dati.\n",
        "    *   Successivamente, sullo **stesso grafico**, useremo `plt.plot()` con i punti `x_line` e `y_line` per sovrapporre la retta di regressione.\n",
        "\n",
        "3.  **Annotare il Grafico:**\n",
        "    *   Un buon grafico scientifico non solo mostra i dati, ma riporta anche i risultati chiave dell'analisi. Aggiungeremo una casella di testo direttamente sul grafico per visualizzare l'equazione della retta e il valore di R², rendendo la figura auto-esplicativa."
      ],
      "metadata": {
        "id": "0BJ0z2gAkh7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assicuriamoci che i risultati della regressione esistano\n",
        "if 'slope' in locals():\n",
        "\n",
        "    # --- Visualizzazione della Retta di Regressione ---\n",
        "\n",
        "    # 1. Generiamo i punti della retta di regressione\n",
        "    # Creiamo un array x per la linea (dai valori min e max dei nostri dati SiO2)\n",
        "    x_line = np.array([x_data.min(), x_data.max()])\n",
        "    # Calcoliamo i corrispondenti valori y usando l'equazione della retta\n",
        "    y_line = slope * x_line + intercept\n",
        "\n",
        "    # 2. Creiamo di nuovo lo scatter plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(x_data, y_data, alpha=0.3, label='Dati Originali')\n",
        "\n",
        "    # 3. Sovrapponiamo la retta di regressione\n",
        "    plt.plot(x_line, y_line, color='red', linestyle='--', linewidth=2, label='Retta di Regressione')\n",
        "\n",
        "    # 4. Aggiungiamo le personalizzazioni\n",
        "    plt.title(\"Diagramma di Harker con Retta di Regressione\")\n",
        "    plt.xlabel(\"SiO₂ (%)\")\n",
        "    plt.ylabel(\"MgO (%)\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Aggiungiamo una casella di testo con i risultati della regressione\n",
        "    # La stringa 'f-string' multilinea è molto comoda per questo\n",
        "    stats_text = (\n",
        "        f\"y = {slope:.2f}x + {intercept:.2f}\\n\"\n",
        "        f\"$R^2$ = {r_value**2:.2f}\"  # Usiamo il formato LaTeX per R^2\n",
        "    )\n",
        "    # 'transform=ax.transAxes' posiziona il testo in coordinate relative al grafico\n",
        "    plt.gca().text(0.65, 0.85, stats_text, transform=plt.gca().transAxes,\n",
        "                   fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Esegui prima la cella precedente per calcolare i parametri della regressione.\")"
      ],
      "metadata": {
        "id": "9jQM1d-rkNrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analisi Avanzata: Regressione Lineare per Gruppo**\n",
        "\n",
        "Abbiamo calcolato una singola retta di regressione per l'intero dataset. Tuttavia, dal nostro grafico a dispersione categoriale, abbiamo notato che le diverse famiglie di rocce (`rock_category`) occupano zone diverse del diagramma di Harker.\n",
        "\n",
        "Una domanda più interessante è: **ogni categoria di roccia segue un proprio trend di differenziazione?**\n",
        "\n",
        "Per rispondere, possiamo combinare la potenza di `.groupby()` con `linregress`. La nostra strategia sarà:\n",
        "\n",
        "1.  **Raggruppare i Dati:** Useremo `df_clean.groupby('rock_category')` per creare gruppi distinti per ogni tipo di roccia (Basaltic, Andesitic, etc.).\n",
        "2.  **Iterare sui Gruppi:** Creeremo un ciclo `for` che \"visita\" ogni gruppo uno per uno.\n",
        "3.  **Calcolare la Regressione per Ogni Gruppo:** All'interno del ciclo, eseguiremo `linregress` solo sui dati del gruppo corrente.\n",
        "4.  **Visualizzare i Risultati:** Creeremo un unico grafico a dispersione con tutti i punti colorati per categoria (come abbiamo fatto prima) e poi, all'interno dello stesso ciclo, sovrapporremo la retta di regressione specifica per ogni gruppo, usando lo stesso colore.\n",
        "\n",
        "Questo ci permetterà di confrontare visivamente i trend di differenziazione delle diverse famiglie magmatiche."
      ],
      "metadata": {
        "id": "qeLefMJfk2dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importiamo la funzione, se non già fatto\n",
        "from scipy.stats import linregress\n",
        "\n",
        "# Assicuriamoci che df_clean e la colonna 'rock_category' esistano\n",
        "if 'balanced_sample' in locals() and 'rock_category' in balanced_sample.columns:\n",
        "\n",
        "    # --- Creazione del Grafico a Dispersione Categoriale Iniziale ---\n",
        "    fig, ax = plt.subplots(figsize=(12, 9))\n",
        "\n",
        "    categories = balanced_sample['rock_category'].unique()\n",
        "\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(categories)))\n",
        "    color_map = dict(zip(categories, colors))\n",
        "\n",
        "    # Plottiamo prima tutti i punti per avere lo sfondo\n",
        "    for category in categories:\n",
        "        subset = balanced_sample[balanced_sample['rock_category'] == category]\n",
        "        ax.scatter(subset['sio2n'], subset['mgon'],\n",
        "                   label=category,\n",
        "                   color=color_map[category],\n",
        "                   alpha=0.4) # Alpha più basso per i punti\n",
        "\n",
        "    # --- Calcolo e Plot della Regressione per Ogni Gruppo ---\n",
        "    print(\"--- Risultati della Regressione per Gruppo ---\")\n",
        "\n",
        "    # Raggruppiamo per categoria di roccia\n",
        "    grouped = balanced_sample.groupby('rock_category')\n",
        "\n",
        "    for name, group in grouped:\n",
        "        # Pulisci i dati per la regressione (rimuovi NaN solo per le colonne di interesse)\n",
        "        regression_data = group[['sio2n', 'mgon']].dropna()\n",
        "\n",
        "        # Esegui la regressione solo se ci sono abbastanza punti (es. > 10)\n",
        "        if len(regression_data) > 10:\n",
        "            x_data = regression_data['sio2n']\n",
        "            y_data = regression_data['mgon']\n",
        "\n",
        "            slope, intercept, r_value, p_value, std_err = linregress(x_data, y_data)\n",
        "\n",
        "            # Stampa i risultati per questo gruppo\n",
        "            print(f\"\\nGruppo: {name}\")\n",
        "            print(f\"  R-squared: {r_value**2:.2f}, P-value: {p_value:.2e}\")\n",
        "\n",
        "            # Genera i punti per la retta di regressione\n",
        "            x_line = np.array([x_data.min(), x_data.max()])\n",
        "            y_line = slope * x_line + intercept\n",
        "\n",
        "            # Sovrapponi la retta al grafico, usando lo stesso colore dei punti\n",
        "            ax.plot(x_line, y_line,\n",
        "                    color=color_map[name],\n",
        "                    linestyle='-',\n",
        "                    linewidth=2.5)\n",
        "\n",
        "    # Personalizzazione finale del grafico\n",
        "    ax.set_title(\"Diagramma di Harker con Rette di Regressione per Categoria\")\n",
        "    ax.set_xlabel(\"SiO₂ (%)\")\n",
        "    ax.set_ylabel(\"MgO (%)\")\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "    ax.legend(title=\"Categoria di Roccia\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame 'df_clean' o colonna 'rock_category' non trovati. Esegui le celle precedenti.\")"
      ],
      "metadata": {
        "id": "5BgViqeUk4PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizzare l'Incertezza: Gli Intervalli di Confidenza**\n",
        "\n",
        "Le rette che abbiamo disegnato finora sono le stime \"migliori\". Tuttavia, in statistica è fondamentale comunicare anche l'**incertezza** di questa stima.\n",
        "\n",
        "Se avessimo preso un campione diverso di rocce, la retta sarebbe leggermente diversa. L'**Intervallo di Confidenza al 95%** (Confidence Interval) viene visualizzato come una banda semitrasparente attorno alla retta. Ci dice che, se ripetessimo il campionamento infinite volte, il 95% delle rette calcolate cadrebbe all'interno di quella banda.\n",
        "\n",
        "*   **Banda stretta:** Abbiamo molti dati e la correlazione è forte (siamo sicuri della posizione della retta).\n",
        "*   **Banda larga:** Abbiamo pochi dati o i dati sono molto dispersi (c'è molta incertezza sulla pendenza reale)."
      ],
      "metadata": {
        "id": "yqDRncwQTdG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dietro le Quinte: Calcolare l'Incertezza (Intervallo di Confidenza)**\n",
        "\n",
        "Nel grafico che stiamo per generare, non ci limiteremo a disegnare la retta di regressione. Disegneremo anche una **banda colorata** attorno ad essa, che rappresenta l'**Intervallo di Confidenza al 95%**.\n",
        "\n",
        "Questa bandanon **non avrà una larghezza costante**: sarà più stretta al centro e si allargherà alle estremità (a forma di clessidra o \"farfallino\").\n",
        "\n",
        "#### **Perché ha questa forma?**\n",
        "La regressione lineare funziona come una leva che ruota attorno al centro di massa dei dati (la media di $x$ e la media di $y$).\n",
        "1.  **Al centro:** Siamo molto sicuri della posizione della retta.\n",
        "2.  **Ai bordi:** Piccoli errori nella stima della pendenza si amplificano man mano che ci allontaniamo dal centro, aumentando l'incertezza.\n",
        "\n",
        "#### **La Matematica del Codice**\n",
        "Nel codice sottostante, calcoliamo questa banda \"manualmente\" per ogni punto $x$ della linea usando questa formula statistica per l'intervallo di confidenza $IC$:\n",
        "\n",
        "$$ IC = \\pm t \\cdot s_e \\cdot \\sqrt{\\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}} $$\n",
        "\n",
        "Dove i componenti principali sono:\n",
        "*   **$t$ (t-student):** Un fattore statistico che dipende da quanto \"sicuri\" vogliamo essere (95%).\n",
        "*   **$s_e$ (Standard Error):** Misura quanto i punti reali sono dispersi attorno alla retta. Se i dati sono molto sparpagliati, la banda sarà più larga.\n",
        "*   **$\\frac{1}{n}$:** Dipende dal numero di campioni. Più dati abbiamo, più piccolo è questo termine e più stretta è la banda.\n",
        "*   **$(x - \\bar{x})^2$:** Questo è il termine che crea la forma a clessidra. L'incertezza aumenta col quadrato della distanza dal valore medio ($\\bar{x}$).\n",
        "\n",
        "Useremo la funzione `plt.fill_between()` di Matplotlib per visualizzare l'area compresa tra il valore superiore e inferiore calcolati con questa formula."
      ],
      "metadata": {
        "id": "E1Ev8JVxUuLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "if 'balanced_sample' in locals():\n",
        "\n",
        "    plt.figure(figsize=(12, 9))\n",
        "\n",
        "    # Definiamo le categorie e i colori\n",
        "    categories = balanced_sample['rock_category'].unique()\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(categories)))\n",
        "    color_map = dict(zip(categories, colors))\n",
        "\n",
        "    for category in categories:\n",
        "        subset = balanced_sample[balanced_sample['rock_category'] == category]\n",
        "\n",
        "        # Saltiamo gruppi troppo piccoli\n",
        "        if len(subset) < 10:\n",
        "            continue\n",
        "\n",
        "        x = subset['sio2n']\n",
        "        y = subset['mgon']\n",
        "\n",
        "        # 1. Scatter Plot dei punti\n",
        "        plt.scatter(x, y, label=category, color=color_map[category], alpha=0.3)\n",
        "\n",
        "        # 2. Calcoli per la regressione e l'intervallo di confidenza\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
        "\n",
        "        # Creiamo una sequenza di X per disegnare la linea e la banda\n",
        "        x_line = np.linspace(x.min(), x.max(), 100)\n",
        "        y_line = slope * x_line + intercept\n",
        "\n",
        "        # --- Calcolo della Banda di Confidenza ---\n",
        "        # (Statistica: Errore Standard della stima)\n",
        "        # Calcoliamo i residui e la somma dei quadrati\n",
        "        y_pred_data = slope * x + intercept\n",
        "        residuals = y - y_pred_data\n",
        "        sum_errs = np.sum(residuals**2)\n",
        "        stdev = np.sqrt(sum_errs / (len(y) - 2)) # Deviazione standard dei residui\n",
        "\n",
        "        # Calcoliamo l'intervallo per ogni punto della linea x_line\n",
        "        # Formula dell'intervallo di previsione media\n",
        "        ci = stdev * np.sqrt(1/len(x) + (x_line - x.mean())**2 / np.sum((x - x.mean())**2))\n",
        "\n",
        "        # Valore t-student per il 95% di confidenza\n",
        "        t_stat = stats.t.ppf(0.975, len(x) - 2)\n",
        "\n",
        "        upper_bound = y_line + (t_stat * ci)\n",
        "        lower_bound = y_line - (t_stat * ci)\n",
        "\n",
        "        # 3. Plot della linea\n",
        "        plt.plot(x_line, y_line, color=color_map[category], linewidth=2)\n",
        "\n",
        "        # 4. Plot della banda (fill_between)\n",
        "        plt.fill_between(x_line, lower_bound, upper_bound, color=color_map[category], alpha=0.15)\n",
        "\n",
        "    plt.title(\"Diagramma di Harker con Bande di Confidenza (Calcolo Manuale)\")\n",
        "    plt.xlabel(\"SiO₂ (%)\")\n",
        "    plt.ylabel(\"MgO (%)\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.legend(title=\"Categoria\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "u2faN-7HTubX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunatamente, Python ci offre delle librerie grafiche che fanno in modo automatico tutto quello che abbiamo fatto manualmente nella cella precedente.\n",
        "\n",
        "Per realizzare questo grafico complesso in modo semplice, possiamo infatti utilizzare **Seaborn**, una libreria di visualizzazione statistica basata su Matplotlib che automatizza il calcolo delle regressioni e degli intervalli di confidenza."
      ],
      "metadata": {
        "id": "5vGKoZqcU3AU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Assicuriamoci che balanced_sample esista\n",
        "if 'balanced_sample' in locals():\n",
        "\n",
        "    # Impostiamo lo stile di Seaborn (opzionale, per renderlo più gradevole)\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "    # Usiamo lmplot (Linear Model Plot).\n",
        "    # Fa tutto lui: scatter plot, regressione per gruppi, e bande di confidenza.\n",
        "    g = sns.lmplot(\n",
        "        data=balanced_sample,\n",
        "        x='sio2n',\n",
        "        y='mgon',\n",
        "        hue='rock_category',  # Colora e divide le regressioni per categoria\n",
        "        height=7,             # Altezza della figura\n",
        "        aspect=1.3,           # Larghezza (aspetto)\n",
        "        scatter_kws={'alpha': 0.5, 's': 30}, # Opzioni per i puntini (trasparenza e dimensione)\n",
        "        line_kws={'linewidth': 2},           # Opzioni per le linee\n",
        "        ci=95                 # Confidence Interval al 95% (default)\n",
        "    )\n",
        "\n",
        "    # Personalizziamo i titoli usando l'oggetto 'g' (FacetGrid)\n",
        "    g.set_axis_labels(\"SiO₂ (%)\", \"MgO (%)\")\n",
        "    g.fig.suptitle(\"Regressione Lineare con Intervalli di Confidenza (95%)\", y=1.02, fontsize=16)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame 'balanced_sample' non trovato.\")"
      ],
      "metadata": {
        "id": "Nf8uW8VmTilJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}